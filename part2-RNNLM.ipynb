{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 224D Assignment #2\n",
    "# Part [2]: Recurrent Neural Networks\n",
    "\n",
    "This notebook will provide starter code, testing snippets, and additional guidance for implementing the Recurrent Neural Network Language Model (RNNLM) described in Part 2 of the handout.\n",
    "\n",
    "Please complete parts (a), (b), and (c) of Part 2 before beginning this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "from numpy import *\n",
    "from matplotlib.pyplot import *\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['savefig.dpi'] = 100\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e): Implement a Recurrent Neural Network Language Model\n",
    "\n",
    "Follow the instructions on the handout to implement your model in `rnnlm.py`, then use the code below to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: temporarily setting self.bptt = len(y) = 3 to compute true gradient.\n",
      "grad_check: dJ/dH error norm = 6.621e-08 [ok]\n",
      "    H dims: [50, 50] = 2500 elem\n",
      "grad_check: dJ/dU error norm = 9.449e-10 [ok]\n",
      "    U dims: [10, 50] = 500 elem\n",
      "grad_check: dJ/dL[1] error norm = 1.566e-08 [ok]\n",
      "    L[1] dims: [50] = 50 elem\n",
      "grad_check: dJ/dL[2] error norm = 1.598e-08 [ok]\n",
      "    L[2] dims: [50] = 50 elem\n",
      "grad_check: dJ/dL[3] error norm = 1.249e-08 [ok]\n",
      "    L[3] dims: [50] = 50 elem\n",
      "Reset self.bptt = 1\n"
     ]
    }
   ],
   "source": [
    "from rnnlm import RNNLM\n",
    "# Gradient check on toy data, for speed\n",
    "random.seed(10)\n",
    "wv_dummy = random.randn(10,50)\n",
    "model = RNNLM(L0 = wv_dummy, U0 = wv_dummy,\n",
    "              alpha=0.005, rseed=10, bptt=1)\n",
    "model.grad_check(array([1,2,3]), array([2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: temporarily setting self.bptt = len(y) = 3 to compute true gradient.\n",
      "grad_check: dJ/dcluster_direct error norm = 1.703e-11 [ok]\n",
      "    cluster_direct dims: [10, 2] = 20 elem\n",
      "grad_check: dJ/dword_direct error norm = 3.328e-10 [ok]\n",
      "    word_direct dims: [10, 10] = 100 elem\n",
      "grad_check: dJ/dU error norm = 8.248e-10 [ok]\n",
      "    U dims: [12, 50] = 600 elem\n",
      "grad_check: dJ/dH error norm = 3.202e-09 [ok]\n",
      "    H dims: [50, 50] = 2500 elem\n",
      "grad_check: dJ/dL[1] error norm = 1.535e-09 [ok]\n",
      "    L[1] dims: [50] = 50 elem\n",
      "grad_check: dJ/dL[2] error norm = 9.996e-10 [ok]\n",
      "    L[2] dims: [50] = 50 elem\n",
      "grad_check: dJ/dL[3] error norm = 1.125e-09 [ok]\n",
      "    L[3] dims: [50] = 50 elem\n",
      "Reset self.bptt = 3\n"
     ]
    }
   ],
   "source": [
    "from rnnlm import ExtraCreditRNNLM\n",
    "random.seed(10)\n",
    "wv_dummy = random.randn(10, 50)\n",
    "u0 = random.randn(12, 50)\n",
    "c_words = array([[0, 1, 2, -1, -1, -1, -1], [3, 4, 5, 6, 7, 8, 9]])\n",
    "lcluster = array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1])\n",
    "args = {\"regular\": 1e-3,\"isME\": True, \"ngram\": 3, \"hash_size\": 100, \"bptt\": 3, \"alpha\": 0.005, \"rseed\": 10, \"isCompression\":False, \"compression_size\": 30, \"class_size\": 2, \"U0\": u0, \"Lcluster\": lcluster, \"cfreq\": array([3, 7]), \"cwords\": c_words}\n",
    "\n",
    "model = ExtraCreditRNNLM(L0=wv_dummy, **args)\n",
    "model.grad_check(array([1,2,3]), array([2,3,4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Vocabulary and Load PTB Data\n",
    "\n",
    "We've pre-prepared a list of the vocabulary in the Penn Treebank, along with their absolute counts and unigram frequencies. The document loader code below will \"canonicalize\" words and replace any unknowns with a `\"UUUNKKK\"` token, then convert the data to lists of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retained 2000 words from 38444 (84.00% of all tokens)\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "from data_utils import utils as du\n",
    "import pandas as pd\n",
    "\n",
    "# Load the vocabulary\n",
    "vocab = pd.read_table(\"data/lm/vocab.ptb.txt\", header=None, sep=\"\\s+\",\n",
    "                     index_col=0, names=['count', 'freq'], )\n",
    "\n",
    "# Choose how many top words to keep            \n",
    "vocabsize = 2000\n",
    "num_to_word = dict(enumerate(vocab.index[:vocabsize]))\n",
    "word_to_num = du.invert_dict(num_to_word)\n",
    "##\n",
    "# Below needed for 'adj_loss': DO NOT CHANGE\n",
    "fraction_lost = float(sum([vocab['count'][word] for word in vocab.index\n",
    "                           if (not word in word_to_num) \n",
    "                               and (not word == \"UUUNKKK\")]))\n",
    "fraction_lost /= sum([vocab['count'][word] for word in vocab.index\n",
    "                      if (not word == \"UUUNKKK\")])\n",
    "print \"Retained %d words from %d (%.02f%% of all tokens)\" % (vocabsize, len(vocab),\n",
    "                                                             100*(1-fraction_lost))\n",
    "def getVocabClass(csize = 10, v=vocab, vs=vocabsize):\n",
    "    freq_percent = 0.0\n",
    "    start = 0\n",
    "    lcluster = []\n",
    "    freq_c = zeros(csize)\n",
    "    freq_sum = vocab['count'][:vs].sum()\n",
    "    for i in xrange(vocabsize):\n",
    "        freq_percent += float(vocab['count'][i]) / (freq_sum + 0.0)\n",
    "        if freq_percent > 1.0:\n",
    "            freq_percent = 1.0\n",
    "        if freq_percent > (start + 1.0) / (csize + 0.0):\n",
    "            lcluster.append(start)\n",
    "            freq_c[start] += 1\n",
    "            if start < csize-1:\n",
    "                start += 1\n",
    "        else:\n",
    "            lcluster.append(start)\n",
    "            freq_c[start] += 1\n",
    "    \n",
    "    freq_words = zeros((csize, int(amax(freq_c)+1)))\n",
    "    start = 0\n",
    "    for i in xrange(csize):\n",
    "        #print freq_c[i]\n",
    "        for j in xrange(int(freq_c[i])):\n",
    "            freq_words[i,j] = int(start)\n",
    "            start += 1 \n",
    "        #print freq_words[i, :int(freq_c[i])]\n",
    "    print len(lcluster)\n",
    "    return array(lcluster).astype(int), freq_c.astype(int), freq_words.astype(int)\n",
    "\n",
    "lcluster, freq_c, freq_words = getVocabClass()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the datasets, using the vocabulary in `word_to_num`. Our starter code handles this for you, and also generates lists of lists X and Y, corresponding to input words and target words*. \n",
    "\n",
    "*(Of course, the target words are just the input words, shifted by one position, but it can be cleaner and less error-prone to keep them separate.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big investment banks refused to step up to the plate to support the beleaguered floor traders by buying big blocks of stock , traders say .\n",
      "[   4  147  169  250 1879    7 1224   64    7    1    3    7  456    1    3\n",
      " 1024  255   24  378  147    3    6   67    0  255  138    2    5]\n"
     ]
    }
   ],
   "source": [
    "# Load the training set\n",
    "docs = du.load_dataset('data/lm/ptb-train.txt')\n",
    "S_train = du.docs_to_indices(docs, word_to_num)\n",
    "X_train, Y_train = du.seqs_to_lmXY(S_train)\n",
    "\n",
    "# Load the dev set (for tuning hyperparameters)\n",
    "docs = du.load_dataset('data/lm/ptb-dev.txt')\n",
    "S_dev = du.docs_to_indices(docs, word_to_num)\n",
    "X_dev, Y_dev = du.seqs_to_lmXY(S_dev)\n",
    "\n",
    "# Load the test set (final evaluation only)\n",
    "docs = du.load_dataset('data/lm/ptb-test.txt')\n",
    "S_test = du.docs_to_indices(docs, word_to_num)\n",
    "X_test, Y_test = du.seqs_to_lmXY(S_test)\n",
    "\n",
    "# Display some sample data\n",
    "print \" \".join(d[0] for d in docs[7])\n",
    "print S_test[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (f): Train and evaluate your model\n",
    "\n",
    "When you're able to pass the gradient check, let's run our model on some real language!\n",
    "\n",
    "You should randomly initialize the word vectors as Gaussian noise, i.e. $L_{ij} \\sim \\mathit{N}(0,0.1)$ and $U_{ij} \\sim \\mathit{N}(0,0.1)$; the function `random.randn` may be helpful here.\n",
    "\n",
    "As in Part 1, you should tune hyperparameters to get a good model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: temporarily setting self.bptt = len(y) = 3 to compute true gradient.\n",
      "grad_check: dJ/dcluster_direct error norm = 6.5e-10 [ok]\n",
      "    cluster_direct dims: [200, 10] = 2000 elem\n",
      "grad_check: dJ/dword_direct error norm = 3.257e-10 [ok]\n",
      "    word_direct dims: [200, 200] = 40000 elem\n",
      "grad_check: dJ/dU error norm = 1.359e-09 [ok]\n",
      "    U dims: [210, 70] = 14700 elem\n",
      "grad_check: dJ/dH error norm = 5.141e-10 [ok]\n",
      "    H dims: [70, 70] = 4900 elem\n",
      "grad_check: dJ/dL[1] error norm = 1.6e-10 [ok]\n",
      "    L[1] dims: [70] = 70 elem\n",
      "grad_check: dJ/dL[2] error norm = 1.743e-10 [ok]\n",
      "    L[2] dims: [70] = 70 elem\n",
      "grad_check: dJ/dL[3] error norm = 1.996e-10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [ok]\n",
      "    L[3] dims: [70] = 70 elem\n",
      "Reset self.bptt = 5\n"
     ]
    }
   ],
   "source": [
    "hdim = 70 # dimension of hidden layer = dimension of word vectors\n",
    "cdim = 30\n",
    "csize = 10\n",
    "random.seed(10)\n",
    "L = zeros((vocabsize, hdim)) # replace with random init, \n",
    "                              # or do in RNNLM.__init__()\n",
    "\n",
    "# test parameters; you probably want to change these\n",
    "sigma = 0.1\n",
    "mu = 0\n",
    "L = random.randn(vocabsize, hdim)*sigma + mu\n",
    "U0 = random.randn(vocabsize+csize, hdim)*sigma + mu \n",
    "\n",
    "args = {\"regular\":1e-4,\"isME\": True, \"bptt\": 5, \"alpha\": 0.1, \"rseed\": 10, \"isCompression\":False, \"compression_size\": cdim, \"class_size\": csize, \"U0\": U0, \"Lcluster\": lcluster, \"cfreq\": freq_c, \"cwords\": freq_words}\n",
    "#print model.cdim\n",
    "model = ExtraCreditRNNLM(L0=L, **args)\n",
    "#print model.hsize\n",
    "\n",
    "# Gradient check is going to take a *long* time here\n",
    "# since it's quadratic-time in the number of parameters.\n",
    "# run at your own risk... (but do check this!)\n",
    "model.grad_check(array([1,2,3]), array([2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "\n",
    "##\n",
    "# Pare down to a smaller dataset, for speed\n",
    "# (optional - recommended to not do this for your final model)\n",
    "ntrain = len(Y_train)\n",
    "X = X_train[:ntrain]\n",
    "Y = Y_train[:ntrain]\n",
    "\n",
    "sequence = range(ntrain)\n",
    "def minibatches(k = 5, seq=sequence):\n",
    "    num_batches = len(seq) / k\n",
    "    for i in xrange(num_batches):\n",
    "        yield random.choice(seq, k)\n",
    "\n",
    "def decreaselr(start=0.1, number=10000):\n",
    "    for i in xrange(number):\n",
    "        yield start * (1 - float(i)/float(number))\n",
    "#ten percent of data for tuning\n",
    "ntrain_small = len(Y_train) / 10\n",
    "X_small = X_train[:ntrain_small]\n",
    "Y_small = Y_train[:ntrain_small]\n",
    "\n",
    "print ntrain_small / 5\n",
    "#model_output = model.train_sgd(X_small, Y_small, idxiter=minibatches(5, range(ntrain_small)), printevery= 300, costevery=1000)\n",
    "#L0 = random.randn(vocabsize, 100)*0.1 + 0\n",
    "#model = RNNLM(L0, U0 = L0, alpha=0.05, rseed=10, bptt=1)\n",
    "\n",
    "#model_output = model.train_sgd(X_small, Y_small, idxiter=minibatches(5, range(ntrain_small)), printevery= 200, costevery=500)\n",
    "#model_test = model.train_sgd(X_small, Y_small, idxiter=(5, range(ntrain_small)), printevery=200, costevery=500)\n",
    "\n",
    "\n",
    "#### END YOUR CODE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin SGD...\n",
      "  Seen 0 in 0.01 s\n",
      "  [0]: mean loss 5.43369\n",
      "  Seen 500 in 251.49 s\n",
      "  Seen 1000 in 294.29 s\n",
      "  [1000]: mean loss 50.9193\n",
      "  Seen 1500 in 538.14 s\n",
      "  Seen 2000 in 580.21 s\n",
      "SGD Interrupted: saw 2000 examples in 772.82 seconds.\n"
     ]
    }
   ],
   "source": [
    "model_output = model.train_sgd(X, Y, idxiter=minibatches(5, range(ntrain)), printevery=500, costevery=1000)\n",
    "train_loss = model.compute_mean_loss(X, Y)\n",
    "dev_loss = model.compute_mean_loss(X_dev, Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begining tuning the proper bptt\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 7.6893\n",
      "  Seen 200 in 83.23 s\n",
      "  Seen 400 in 123.33 s\n",
      "  [500]: mean loss 5.0037\n",
      "  Seen 600 in 206.51 s\n",
      "  Seen 800 in 247.07 s\n",
      "  Seen 1000 in 287.79 s\n",
      "  [1000]: mean loss 4.7962\n",
      "  [1130]: mean loss 4.73605\n",
      "SGD complete: 1130 examples in 403.91 seconds.\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 7.70895\n",
      "  Seen 200 in 88.01 s\n",
      "  Seen 400 in 131.37 s\n",
      "  [500]: mean loss 6.42164\n",
      "  Seen 600 in 217.84 s\n",
      "  Seen 800 in 259.28 s\n",
      "  Seen 1000 in 302.29 s\n",
      "  [1000]: mean loss 5.14403\n",
      "  [1130]: mean loss 4.91755\n",
      "SGD complete: 1130 examples in 417.05 seconds.\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 7.66984\n",
      "  Seen 200 in 86.46 s\n",
      "  Seen 400 in 129.71 s\n",
      "  [500]: mean loss 5.00425\n",
      "  Seen 600 in 213.09 s\n",
      "  Seen 800 in 254.63 s\n",
      "  Seen 1000 in 296.11 s\n",
      "  [1000]: mean loss 4.82172\n",
      "  [1130]: mean loss 4.82933\n",
      "SGD complete: 1130 examples in 408.33 seconds.\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 7.6686\n",
      "  Seen 200 in 85.90 s\n",
      "  Seen 400 in 128.85 s\n",
      "  [500]: mean loss 5.29079\n",
      "  Seen 600 in 214.28 s\n",
      "  Seen 800 in 259.08 s\n",
      "  Seen 1000 in 304.50 s\n",
      "  [1000]: mean loss 4.87529\n",
      "  [1130]: mean loss 4.97775\n",
      "SGD complete: 1130 examples in 421.20 seconds.\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 7.6313\n",
      "  Seen 200 in 96.17 s\n",
      "  Seen 400 in 142.47 s\n",
      "  [500]: mean loss 6.77091\n",
      "  Seen 600 in 233.74 s\n",
      "  Seen 800 in 277.71 s\n",
      "  Seen 1000 in 324.90 s\n",
      "  [1000]: mean loss 4.94892\n",
      "  [1130]: mean loss 4.91385\n",
      "SGD complete: 1130 examples in 440.63 seconds.\n",
      "Endding tuning the proper bptt\n"
     ]
    }
   ],
   "source": [
    "bptts = range(1, 6)\n",
    "optim_bptt = 5\n",
    "loss_best = 10\n",
    "first = True\n",
    "print \"begining tuning the proper bptt\"\n",
    "for i in bptts:\n",
    "    model = RNNLM(L0, U0 = L0, alpha=0.1, rseed=10, bptt=i)\n",
    "    model_output = model.train_sgd(X_small, Y_small, idxiter=minibatches(5, range(ntrain_small)), printevery=200, costevery=500)\n",
    "    \n",
    "    if first:\n",
    "        loss_best = model_output\n",
    "        first = False\n",
    "    else:\n",
    "        if loss_best > model_output:\n",
    "            loss_best = model_output\n",
    "            optim_bptt = i\n",
    "print \"Endding tuning the proper bptt\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 5.49968\n",
      "  Seen 1000 in 323.15 s\n",
      "  Seen 2000 in 472.09 s\n",
      "  Seen 3000 in 618.96 s\n",
      "  Seen 4000 in 765.11 s\n",
      "  Seen 5000 in 909.97 s\n",
      "  [5000]: mean loss 5.43254\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6fe54ccddd34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_sgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mntrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprintevery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcostevery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mean_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdev_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mean_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jan/course/cs224d/assignments/assignment2/nn/base.pyc\u001b[0m in \u001b[0;36mtrain_sgd\u001b[0;34m(self, X, y, idxiter, alphaiter, printevery, costevery, devidx)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevidx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_display_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdevidx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdevidx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_display_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m         \u001b[0mcosts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"  [%d]: mean loss %g\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jan/course/cs224d/assignments/assignment2/nn/base.pyc\u001b[0m in \u001b[0;36mcompute_display_loss\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mOptional\u001b[0m \u001b[0malternative\u001b[0m \u001b[0mloss\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mprinting\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdiagnostics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \"\"\"\n\u001b[0;32m--> 413\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mean_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m     def train_sgd(self, X, y,\n",
      "\u001b[0;32m/Users/jan/course/cs224d/assignments/assignment2/rnnlm.py\u001b[0m in \u001b[0;36mcompute_mean_loss\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0mDo\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodify\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;31m!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \"\"\"\n\u001b[0;32m--> 551\u001b[0;31m         \u001b[0mJ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m         \u001b[0mntot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mJ\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mntot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jan/course/cs224d/assignments/assignment2/rnnlm.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# multiple examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             return sum([self.compute_seq_loss(xs,ys)\n\u001b[0;32m--> 543\u001b[0;31m                        for xs,ys in itertools.izip(X, Y)])\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_mean_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jan/course/cs224d/assignments/assignment2/rnnlm.py\u001b[0m in \u001b[0;36mcompute_seq_loss\u001b[0;34m(self, xs, ys)\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0mcs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvdim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvdim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                 \u001b[0mps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mst_word\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0med_word\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mst_word\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0med_word\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m                 \u001b[0;31m#print ps[i, st_word:ed_word]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_output = model.train_sgd(X, Y, idxiter=minibatches(5, range(ntrain)), printevery=1000, costevery=5000)\n",
    "train_loss = model.compute_mean_loss(X, Y)\n",
    "dev_loss = model.compute_mean_loss(X_dev, Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unadjusted: 71.584\n",
      "Adjusted for missing vocab: 113.881\n"
     ]
    }
   ],
   "source": [
    "def adjust_loss(loss, funk, q, mode='basic'):\n",
    "    if mode == 'basic':\n",
    "        # remove freebies only: score if had no UUUNKKK\n",
    "        return (loss + funk*log(funk))/(1 - funk)\n",
    "    else:\n",
    "        # remove freebies, replace with best prediction on remaining\n",
    "        return loss + funk*log(funk) - funk*log(q)\n",
    "# q = best unigram frequency from omitted vocab\n",
    "# this is the best expected loss out of that set\n",
    "q = vocab.freq[vocabsize] / sum(vocab.freq[vocabsize:])\n",
    "print \"Unadjusted: %.03f\" % exp(dev_loss)\n",
    "print \"Adjusted for missing vocab: %.03f\" % exp(adjust_loss(dev_loss, fraction_lost, q, mode=\"basic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 5.59847\n",
      "  Seen 1000 in 295.52 s\n",
      "  Seen 2000 in 391.79 s\n",
      "  Seen 3000 in 488.52 s\n",
      "  Seen 4000 in 585.40 s\n",
      "  Seen 5000 in 681.73 s\n",
      "  [5000]: mean loss 4.29344\n",
      "  Seen 6000 in 976.79 s\n",
      "  Seen 7000 in 1073.25 s\n",
      "  Seen 8000 in 1169.72 s\n",
      "  Seen 9000 in 1266.09 s\n",
      "  Seen 10000 in 1362.60 s\n",
      "  [10000]: mean loss 4.12286\n",
      "  Seen 11000 in 1660.99 s\n",
      "  [11304]: mean loss 4.07628\n",
      "SGD complete: 11304 examples in 1897.84 seconds.\n",
      "Unadjusted: 61.800\n",
      "Adjusted for missing vocab: 95.603\n"
     ]
    }
   ],
   "source": [
    "hdim = 80 # dimension of hidden layer = dimension of word vectors\n",
    "csize = 10\n",
    "random.seed(10)\n",
    "L = zeros((vocabsize, hdim)) # replace with random init, \n",
    "                              # or do in RNNLM.__init__()\n",
    "\n",
    "# test parameters; you probably want to change these\n",
    "sigma = 0.1\n",
    "mu = 0\n",
    "L = random.randn(vocabsize, hdim)*sigma + mu\n",
    "U0 = random.randn(vocabsize+csize, hdim)*sigma + mu \n",
    "\n",
    "args = {\"isME\": True,\"bptt\": 5, \"alpha\": 0.1, \"rseed\": 10, \"isCompression\":False, \"class_size\": csize, \"U0\": U0, \"Lcluster\": lcluster, \"cfreq\": freq_c, \"cwords\": freq_words}\n",
    "\n",
    "model = ExtraCreditRNNLM(L0=L, **args)\n",
    "\n",
    "model_output = model.train_sgd(X, Y, idxiter=minibatches(5, range(ntrain)), printevery=1000, costevery=5000)\n",
    "train_loss = model.compute_mean_ppl(X, Y)\n",
    "dev_loss = model.compute_mean_ppl(X_dev, Y_dev)\n",
    "\n",
    "def adjust_loss(loss, funk, q, mode='basic'):\n",
    "    if mode == 'basic':\n",
    "        # remove freebies only: score if had no UUUNKKK\n",
    "        return (loss + funk*log(funk))/(1 - funk)\n",
    "    else:\n",
    "        # remove freebies, replace with best prediction on remaining\n",
    "        return loss + funk*log(funk) - funk*log(q)\n",
    "# q = best unigram frequency from omitted vocab\n",
    "# this is the best expected loss out of that set\n",
    "q = vocab.freq[vocabsize] / sum(vocab.freq[vocabsize:])\n",
    "print \"Unadjusted: %.03f\" % exp(dev_loss)\n",
    "print \"Adjusted for missing vocab: %.03f\" % exp(adjust_loss(dev_loss, fraction_lost, q, mode=\"basic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 5.50063\n",
      "  Seen 1000 in 233.34 s\n",
      "  Seen 2000 in 297.53 s\n",
      "  Seen 3000 in 360.81 s\n",
      "  Seen 4000 in 426.96 s\n",
      "  Seen 5000 in 491.42 s\n",
      "  [5000]: mean loss 5.43527\n",
      "  Seen 6000 in 722.58 s\n",
      "  Seen 7000 in 787.95 s\n",
      "  Seen 8000 in 851.20 s\n",
      "  Seen 9000 in 915.36 s\n",
      "  Seen 10000 in 979.49 s\n",
      "  [10000]: mean loss 5.29684\n",
      "  Seen 11000 in 1207.73 s\n",
      "  [11304]: mean loss 5.46928\n",
      "SGD complete: 11304 examples in 1391.36 seconds.\n",
      "Unadjusted: 236.207\n",
      "Adjusted for missing vocab: 471.697\n"
     ]
    }
   ],
   "source": [
    "hdim = 80 # dimension of hidden layer = dimension of word vectors\n",
    "cdim = 40\n",
    "csize = 10\n",
    "random.seed(10)\n",
    "L = zeros((vocabsize, hdim)) # replace with random init, \n",
    "                              # or do in RNNLM.__init__()\n",
    "\n",
    "# test parameters; you probably want to change these\n",
    "sigma = 0.1\n",
    "mu = 0\n",
    "L = random.randn(vocabsize, hdim)*sigma + mu\n",
    "U0 = random.randn(vocabsize+csize, cdim)*sigma + mu \n",
    "\n",
    "args = {\"bptt\": 5, \"alpha\": 0.1, \"rseed\": 30, \"isCompression\":True, \"compression_size\": cdim, \"class_size\": csize, \"U0\": U0, \"Lcluster\": lcluster, \"cfreq\": freq_c, \"cwords\": freq_words} \n",
    "\n",
    "#print \"Finishing args setting\"\n",
    "model = ExtraCreditRNNLM(L0=L, **args)\n",
    "#print \"Starting to train model\"\n",
    "model_output = model.train_sgd(X, Y, idxiter=minibatches(5, range(ntrain)), printevery=1000, costevery=5000)\n",
    "train_loss = model.compute_mean_ppl(X, Y)\n",
    "dev_loss = model.compute_mean_ppl(X_dev, Y_dev)\n",
    "\n",
    "def adjust_loss(loss, funk, q, mode='basic'):\n",
    "    if mode == 'basic':\n",
    "        # remove freebies only: score if had no UUUNKKK\n",
    "        return (loss + funk*log(funk))/(1 - funk)\n",
    "    else:\n",
    "        # remove freebies, replace with best prediction on remaining\n",
    "        return loss + funk*log(funk) - funk*log(q)\n",
    "# q = best unigram frequency from omitted vocab\n",
    "# this is the best expected loss out of that set\n",
    "q = vocab.freq[vocabsize] / sum(vocab.freq[vocabsize:])\n",
    "print \"Unadjusted: %.03f\" % exp(dev_loss)\n",
    "print \"Adjusted for missing vocab: %.03f\" % exp(adjust_loss(dev_loss, fraction_lost, q, mode=\"basic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 7.5281\n",
      "  Seen 1000 in 659.80 s\n",
      "  Seen 2000 in 889.35 s\n",
      "  Seen 3000 in 1118.35 s\n",
      "  Seen 4000 in 1343.70 s\n",
      "  Seen 5000 in 1570.45 s\n",
      "  [5000]: mean loss 4.42817\n",
      "  Seen 6000 in 2227.53 s\n",
      "  Seen 7000 in 2454.10 s\n",
      "  Seen 8000 in 2684.65 s\n",
      "  Seen 9000 in 2911.24 s\n",
      "  Seen 10000 in 3141.05 s\n",
      "  [10000]: mean loss 4.27154\n",
      "  Seen 11000 in 3800.62 s\n",
      "  [11304]: mean loss 4.2159\n",
      "SGD complete: 11304 examples in 4296.65 seconds.\n"
     ]
    }
   ],
   "source": [
    "## Evaluate cross-entropy loss on the dev set,\n",
    "## then convert to perplexity for your writeup\n",
    "optim_dim = 100\n",
    "optim_bptt = 5\n",
    "L0 = random.randn(vocabsize, optim_dim) * sigma + mu\n",
    "model = RNNLM(L0, U0 = L0, alpha=0.1, rseed=10, bptt=optim_bptt)\n",
    "model_output = model.train_sgd(X, Y, idxiter=minibatches(5, range(ntrain)), printevery=1000, costevery=5000)\n",
    "dev_loss = model.compute_mean_loss(X_dev, Y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the model is skewed somewhat by the large number of `UUUNKKK` tokens; if these are 1/6 of the dataset, then that's a sizeable fraction that we're just waving our hands at. Naively, our model gets credit for these that's not really deserved; the formula below roughly removes this contribution from the average loss. Don't worry about how it's derived, but do report both scores - it helps us compare across models with different vocabulary sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unadjusted: 68.019\n",
      "Adjusted for missing vocab: 107.163\n"
     ]
    }
   ],
   "source": [
    "## DO NOT CHANGE THIS CELL ##\n",
    "# Report your numbers, after computing dev_loss above.\n",
    "#without compression layer\n",
    "def adjust_loss(loss, funk, q, mode='basic'):\n",
    "    if mode == 'basic':\n",
    "        # remove freebies only: score if had no UUUNKKK\n",
    "        return (loss + funk*log(funk))/(1 - funk)\n",
    "    else:\n",
    "        # remove freebies, replace with best prediction on remaining\n",
    "        return loss + funk*log(funk) - funk*log(q)\n",
    "# q = best unigram frequency from omitted vocab\n",
    "# this is the best expected loss out of that set\n",
    "q = vocab.freq[vocabsize] / sum(vocab.freq[vocabsize:])\n",
    "print \"Unadjusted: %.03f\" % exp(dev_loss)\n",
    "print \"Adjusted for missing vocab: %.03f\" % exp(adjust_loss(dev_loss, fraction_lost, q, mode=\"basic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unadjusted: 58.317\n",
      "Adjusted for missing vocab: 89.224\n"
     ]
    }
   ],
   "source": [
    "## DO NOT CHANGE THIS CELL ##\n",
    "# Report your numbers, after computing dev_loss above.\n",
    "#with compression layer, csize=10, vocab = 2000, cdim=50\n",
    "def adjust_loss(loss, funk, q, mode='basic'):\n",
    "    if mode == 'basic':\n",
    "        # remove freebies only: score if had no UUUNKKK\n",
    "        return (loss + funk*log(funk))/(1 - funk)\n",
    "    else:\n",
    "        # remove freebies, replace with best prediction on remaining\n",
    "        return loss + funk*log(funk) - funk*log(q)\n",
    "# q = best unigram frequency from omitted vocab\n",
    "# this is the best expected loss out of that set\n",
    "q = vocab.freq[vocabsize] / sum(vocab.freq[vocabsize:])\n",
    "print \"Unadjusted: %.03f\" % exp(dev_loss)\n",
    "print \"Adjusted for missing vocab: %.03f\" % exp(adjust_loss(dev_loss, fraction_lost, q, mode=\"basic\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "# Save to .npy files; should only be a few MB total\n",
    "assert(min(model.sparams.L.shape) <= 100) # don't be too big\n",
    "assert(max(model.sparams.L.shape) <= 5000) # don't be too big\n",
    "save(\"rnnlm.L2.npy\", model.sparams.L)\n",
    "save(\"rnnlm.U2.npy\", model.params.U)\n",
    "save(\"rnnlm.H2.npy\", model.params.H)\n",
    "save(\"rnnlm.C2.npy\", model.params.C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (g): Generating Data\n",
    "\n",
    "Once you've trained your model to satisfaction, let's use it to generate some sentences!\n",
    "\n",
    "Implement the `generate_sequence` function in `rnnlm.py`, and call it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117.591262186\n",
      "<s> the mortgage regulatory UUUNKKK no institute in the japan ) acquisition would be buying UUUNKKK -- america on UUUNKKK 's UUUNKKK , '' he added . </s>\n"
     ]
    }
   ],
   "source": [
    "def seq_to_words(seq):\n",
    "    return [num_to_word[s] for s in seq]\n",
    "    \n",
    "seq, J = model.generate_sequence(word_to_num[\"<s>\"], \n",
    "                                 word_to_num[\"</s>\"], \n",
    "                                 maxlen=100)\n",
    "print J\n",
    "# print seq\n",
    "print \" \".join(seq_to_words(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BONUS:** Use the unigram distribution given in the `vocab` table to fill in any `UUUNKKK` tokens in your generated sequences with words that we omitted from the vocabulary. You'll want to use `list(vocab.index)` to get a list of words, and `vocab.freq` to get a list of corresponding frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> the mortgage regulatory reform no institute in the japan ) acquisition would be buying DGDGDGDG -- america on damage 's funding , '' he added . </s>\n"
     ]
    }
   ],
   "source": [
    "# Replace UUUNKKK with a random unigram,\n",
    "# drawn from vocab that we skipped\n",
    "from nn.math import MultinomialSampler, multinomial_sample\n",
    "def fill_unknowns(words):\n",
    "    #### YOUR CODE HERE ####\n",
    "    sum_ = sum(vocab.freq)\n",
    "    unigram = vocab.freq / float(sum_)    \n",
    "    ret = words # do nothing; replace this\n",
    "    for i in xrange(len(words)):\n",
    "        if ret[i] == \"UUUNKKK\":\n",
    "            rep = multinomial_sample(unigram)\n",
    "            ret[i] = list(vocab.index)[rep]\n",
    "    \n",
    "\n",
    "    #### END YOUR CODE ####\n",
    "    return ret\n",
    "    \n",
    "print \" \".join(fill_unknowns(seq_to_words(seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
